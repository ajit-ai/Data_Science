{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7bb318",
   "metadata": {},
   "source": [
    "I'll provide a comprehensive explanation of Restricted Boltzmann Machines (RBMs), including their use cases, domain, and a Python implementation using NumPy. I'll also explain the code and provide a clear understanding of RBMs, their applications, and their mathematical foundation.\n",
    "\n",
    "---\n",
    "\n",
    "### What is a Restricted Boltzmann Machine (RBM)?\n",
    "\n",
    "A **Restricted Boltzmann Machine (RBM)** is a generative stochastic artificial neural network used for unsupervised learning. It consists of two layers:\n",
    "- **Visible Layer**: Represents the input data.\n",
    "- **Hidden Layer**: Captures latent features or patterns in the data.\n",
    "\n",
    "The \"restricted\" part refers to the constraint that there are no connections within the same layer (i.e., no visible-to-visible or hidden-to-hidden connections), making it a bipartite graph. RBMs are used to model the probability distribution of input data and are often employed in tasks like dimensionality reduction, feature learning, and collaborative filtering.\n",
    "\n",
    "RBMs are probabilistic models that learn a joint probability distribution \\( P(v, h) \\) over visible units \\( v \\) and hidden units \\( h \\). They are trained to maximize the likelihood of the data using techniques like **Contrastive Divergence**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Components of RBMs\n",
    "\n",
    "1. **Visible Units (v)**: These represent the input data (e.g., pixel intensities for images, ratings for recommender systems).\n",
    "2. **Hidden Units (h)**: These capture latent features or patterns in the data.\n",
    "3. **Weights (W)**: A weight matrix connecting visible and hidden units.\n",
    "4. **Biases**:\n",
    "   - **Visible Bias (b)**: Bias for visible units.\n",
    "   - **Hidden Bias (c)**: Bias for hidden units.\n",
    "5. **Energy Function**: The energy of a configuration \\((v, h)\\) is defined as:\n",
    "   \\[\n",
    "   E(v, h) = -\\sum_i v_i b_i - \\sum_j h_j c_j - \\sum_{i,j} v_i h_j w_{ij}\n",
    "   \\]\n",
    "   where \\( v_i \\) is the state of visible unit \\( i \\), \\( h_j \\) is the state of hidden unit \\( j \\), \\( b_i \\) is the bias for visible unit \\( i \\), \\( c_j \\) is the bias for hidden unit \\( j \\), and \\( w_{ij} \\) is the weight between visible unit \\( i \\) and hidden unit \\( j \\).\n",
    "6. **Probability Distribution**: The joint probability of visible and hidden units is:\n",
    "   \\[\n",
    "   P(v, h) = \\frac{e^{-E(v, h)}}{Z}\n",
    "   \\]\n",
    "   where \\( Z \\) is the partition function (normalization constant).\n",
    "7. **Training**: RBMs are trained using **Contrastive Divergence (CD)**, which approximates the gradient of the log-likelihood to update weights and biases.\n",
    "\n",
    "---\n",
    "\n",
    "### Use Cases of RBMs\n",
    "\n",
    "RBMs are versatile and have been used in various domains. Below are some key use cases:\n",
    "\n",
    "1. **Recommender Systems**:\n",
    "   - **Domain**: Collaborative filtering (e.g., Netflix, Amazon).\n",
    "   - **Use Case**: RBMs model user-item interactions (e.g., movie ratings) to predict missing ratings or recommend items. Visible units represent user ratings, and hidden units capture latent factors (e.g., genres or user preferences).\n",
    "   - **Example**: Netflix used RBMs in the past to recommend movies based on user ratings.\n",
    "\n",
    "2. **Dimensionality Reduction**:\n",
    "   - **Domain**: Data preprocessing, feature extraction.\n",
    "   - **Use Case**: RBMs reduce high-dimensional data (e.g., images, text) into a lower-dimensional representation by learning latent features in the hidden layer.\n",
    "   - **Example**: Reducing the dimensionality of image data for classification tasks.\n",
    "\n",
    "3. **Feature Learning**:\n",
    "   - **Domain**: Computer vision, natural language processing.\n",
    "   - **Use Case**: RBMs learn meaningful features from raw data (e.g., edge detectors in images or semantic features in text) for downstream tasks like classification or clustering.\n",
    "   - **Example**: Pretraining layers in deep belief networks for image recognition.\n",
    "\n",
    "4. **Image Processing**:\n",
    "   - **Domain**: Computer vision.\n",
    "   - **Use Case**: RBMs can denoise images or reconstruct missing parts of images by modeling the distribution of pixel intensities.\n",
    "   - **Example**: Reconstructing corrupted MNIST handwritten digit images.\n",
    "\n",
    "5. **Anomaly Detection**:\n",
    "   - **Domain**: Cybersecurity, fraud detection.\n",
    "   - **Use Case**: RBMs model normal data distributions and detect anomalies by identifying data points with low probability under the learned distribution.\n",
    "   - **Example**: Detecting unusual network traffic patterns.\n",
    "\n",
    "6. **Natural Language Processing**:\n",
    "   - **Domain**: Text analysis, topic modeling.\n",
    "   - **Use Case**: RBMs can model word distributions to extract topics or features from text data.\n",
    "   - **Example**: Learning latent topics from document-term matrices.\n",
    "\n",
    "---\n",
    "\n",
    "### Domains Where RBMs Are Applied\n",
    "\n",
    "- **Machine Learning**: RBMs are used in unsupervised learning tasks and as building blocks for deep learning models like Deep Belief Networks (DBNs).\n",
    "- **Data Science**: For data preprocessing, feature extraction, and visualization.\n",
    "- **Computer Vision**: For image denoising, reconstruction, and feature learning.\n",
    "- **Recommender Systems**: For collaborative filtering in e-commerce and media platforms.\n",
    "- **Natural Language Processing**: For topic modeling and text feature extraction.\n",
    "- **Cybersecurity**: For anomaly detection and pattern recognition.\n",
    "\n",
    "---\n",
    "\n",
    "### Python Implementation of RBM\n",
    "\n",
    "Below is a Python implementation of a binary RBM (where visible and hidden units are binary) using NumPy. The implementation includes training with Contrastive Divergence (CD-k) and sampling for inference.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.01, k=1):\n",
    "        \"\"\"\n",
    "        Initialize a Restricted Boltzmann Machine.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_visible: Number of visible units\n",
    "        - n_hidden: Number of hidden units\n",
    "        - learning_rate: Learning rate for weight updates\n",
    "        - k: Number of Gibbs sampling steps for Contrastive Divergence (CD-k)\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.k = k\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.normal(0, 0.01, (n_visible, n_hidden))  # Weight matrix\n",
    "        self.b = np.zeros(n_visible)  # Visible biases\n",
    "        self.c = np.zeros(n_hidden)   # Hidden biases\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Compute sigmoid activation.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sample_hidden(self, v):\n",
    "        \"\"\"\n",
    "        Sample hidden units given visible units.\n",
    "        Returns: Probabilities and binary states of hidden units.\n",
    "        \"\"\"\n",
    "        activation = np.dot(v, self.W) + self.c\n",
    "        p_h = self.sigmoid(activation)\n",
    "        h = (p_h > np.random.random(p_h.shape)).astype(float)\n",
    "        return p_h, h\n",
    "\n",
    "    def sample_visible(self, h):\n",
    "        \"\"\"\n",
    "        Sample visible units given hidden units.\n",
    "        Returns: Probabilities and binary states of visible units.\n",
    "        \"\"\"\n",
    "        activation = np.dot(h, self.W.T) + self.b\n",
    "        p_v = self.sigmoid(activation)\n",
    "        v = (p_v > np.random.random(p_v.shape)).astype(float)\n",
    "        return p_v, v\n",
    "\n",
    "    def contrastive_divergence(self, v):\n",
    "        \"\"\"\n",
    "        Perform one step of Contrastive Divergence (CD-k).\n",
    "        Returns: Positive and negative gradients for weights and biases.\n",
    "        \"\"\"\n",
    "        # Positive phase\n",
    "        pos_h_prob, pos_h = self.sample_hidden(v)\n",
    "        pos_assoc = np.dot(v.T, pos_h_prob)\n",
    "\n",
    "        # Negative phase (k steps of Gibbs sampling)\n",
    "        neg_v = v.copy()\n",
    "        for _ in range(self.k):\n",
    "            neg_h_prob, neg_h = self.sample_hidden(neg_v)\n",
    "            neg_v_prob, neg_v = self.sample_visible(neg_h)\n",
    "        \n",
    "        neg_assoc = np.dot(neg_v.T, neg_h_prob)\n",
    "\n",
    "        # Gradients\n",
    "        grad_W = pos_assoc - neg_assoc\n",
    "        grad_b = np.mean(v - neg_v, axis=0)\n",
    "        grad_c = np.mean(pos_h_prob - neg_h_prob, axis=0)\n",
    "\n",
    "        return grad_W, grad_b, grad_c\n",
    "\n",
    "    def train(self, data, epochs=100, batch_size=100):\n",
    "        \"\"\"\n",
    "        Train the RBM using Contrastive Divergence.\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Training data (binary, shape: [n_samples, n_visible])\n",
    "        - epochs: Number of training epochs\n",
    "        - batch_size: Size of mini-batches\n",
    "        \"\"\"\n",
    "        n_samples = data.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            error = 0\n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = min(start + batch_size, n_samples)\n",
    "                batch = data[start:end]\n",
    "\n",
    "                # Compute gradients\n",
    "                grad_W, grad_b, grad_c = self.contrastive_divergence(batch)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.W += self.learning_rate * grad_W / batch_size\n",
    "                self.b += self.learning_rate * grad_b\n",
    "                self.c += self.learning_rate * grad_c\n",
    "\n",
    "                # Compute reconstruction error\n",
    "                error += np.mean((batch - self.sample_visible(self.sample_hidden(batch)[1])[1])**2)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Reconstruction Error: {error / (n_samples // batch_size):.4f}\")\n",
    "\n",
    "    def reconstruct(self, v):\n",
    "        \"\"\"\n",
    "        Reconstruct input data by sampling hidden and then visible units.\n",
    "        Returns: Reconstructed visible units.\n",
    "        \"\"\"\n",
    "        _, h = self.sample_hidden(v)\n",
    "        _, v_reconstructed = self.sample_visible(h)\n",
    "        return v_reconstructed\n",
    "\n",
    "# Example usage with synthetic binary data\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic binary data (e.g., 100 samples with 10 visible units)\n",
    "    np.random.seed(42)\n",
    "    data = (np.random.random((100, 10)) > 0.5).astype(float)\n",
    "    \n",
    "    # Initialize and train RBM\n",
    "    rbm = RBM(n_visible=10, n_hidden=5, learning_rate=0.1, k=1)\n",
    "    rbm.train(data, epochs=100, batch_size=10)\n",
    "    \n",
    "    # Reconstruct a sample\n",
    "    sample = data[0:1]\n",
    "    reconstructed = rbm.reconstruct(sample)\n",
    "    print(\"Original:\", sample)\n",
    "    print(\"Reconstructed:\", reconstructed)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "#### 1. **Class Initialization**:\n",
    "- **Parameters**:\n",
    "  - `n_visible`: Number of visible units (size of input data).\n",
    "  - `n_hidden`: Number of hidden units (size of latent representation).\n",
    "  - `learning_rate`: Step size for weight and bias updates.\n",
    "  - `k`: Number of Gibbs sampling steps for Contrastive Divergence (CD-k).\n",
    "- **Weights and Biases**:\n",
    "  - `W`: Weight matrix initialized with small random values (normal distribution).\n",
    "  - `b`: Visible biases initialized to zero.\n",
    "  - `c`: Hidden biases initialized to zero.\n",
    "\n",
    "#### 2. **Sigmoid Function**:\n",
    "- Computes the sigmoid activation, used to calculate probabilities for binary units:\n",
    "  \\[\n",
    "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "  \\]\n",
    "\n",
    "#### 3. **Sampling Functions**:\n",
    "- **sample_hidden(v)**: Given visible units, compute hidden unit probabilities and sample binary states.\n",
    "  \\[\n",
    "  P(h_j = 1 | v) = \\sigma(\\sum_i v_i w_{ij} + c_j)\n",
    "  \\]\n",
    "- **sample_visible(h)**: Given hidden units, compute visible unit probabilities and sample binary states.\n",
    "  \\[\n",
    "  P(v_i = 1 | h) = \\sigma(\\sum_j h_j w_{ij} + b_i)\n",
    "  \\]\n",
    "\n",
    "#### 4. **Contrastive Divergence (CD-k)**:\n",
    "- **Positive Phase**: Compute hidden unit probabilities from input data and calculate the positive association (\\( v^T h \\)).\n",
    "- **Negative Phase**: Perform \\( k \\) steps of Gibbs sampling to reconstruct visible units and compute negative association.\n",
    "- **Gradients**:\n",
    "  - Weight gradient: Difference between positive and negative associations.\n",
    "  - Bias gradients: Mean differences for visible and hidden units.\n",
    "\n",
    "#### 5. **Training**:\n",
    "- Iterates over the data in mini-batches for a specified number of epochs.\n",
    "- Updates weights and biases using gradients from Contrastive Divergence.\n",
    "- Computes reconstruction error to monitor training progress.\n",
    "\n",
    "#### 6. **Reconstruction**:\n",
    "- Takes input data, samples hidden units, and reconstructs visible units to test the learned model.\n",
    "\n",
    "#### 7. **Example Usage**:\n",
    "- Generates synthetic binary data (100 samples, 10 visible units).\n",
    "- Trains an RBM with 5 hidden units.\n",
    "- Reconstructs a sample to demonstrate the modelâ€™s ability to learn patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case Example: Recommender System\n",
    "\n",
    "Suppose you want to use an RBM for a movie recommendation system:\n",
    "- **Visible Units**: Represent user ratings for movies (e.g., binary: 1 for \"liked,\" 0 for \"not rated\").\n",
    "- **Hidden Units**: Capture latent factors like genres or user preferences.\n",
    "- **Training**: Train the RBM on a user-movie rating matrix.\n",
    "- **Inference**: For a user, input their ratings, sample hidden units, and reconstruct visible units to predict missing ratings.\n",
    "\n",
    "**Example**:\n",
    "- Input: User ratings `[1, 0, 1, 0, 0]` (liked movies 1 and 3, others not rated).\n",
    "- RBM learns latent features (e.g., action vs. drama preferences).\n",
    "- Output: Reconstructed ratings `[1, 0.9, 1, 0.2, 0.8]`, suggesting the user might like movies 2 and 5.\n",
    "\n",
    "---\n",
    "\n",
    "### Domain-Specific Considerations\n",
    "\n",
    "1. **Recommender Systems**:\n",
    "   - **Data**: User-item matrices (sparse, often binary or real-valued).\n",
    "   - **Challenge**: Handling missing data (not rated items).\n",
    "   - **Solution**: RBMs treat missing ratings as zeros or use specialized variants (e.g., Gaussian RBMs for real-valued ratings).\n",
    "\n",
    "2. **Image Processing**:\n",
    "   - **Data**: Pixel intensities (binary for black-and-white, real-valued for grayscale).\n",
    "   - **Challenge**: High dimensionality.\n",
    "   - **Solution**: Use RBMs to learn compact feature representations.\n",
    "\n",
    "3. **Anomaly Detection**:\n",
    "   - **Data**: Time-series or network traffic data.\n",
    "   - **Challenge**: Defining \"normal\" behavior.\n",
    "   - **Solution**: Train RBM on normal data and flag low-probability inputs as anomalies.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of RBMs\n",
    "\n",
    "1. **Scalability**: Training RBMs on large datasets can be computationally expensive due to Gibbs sampling.\n",
    "2. **Approximation**: Contrastive Divergence is an approximation, which may not fully optimize the likelihood.\n",
    "3. **Modern Alternatives**: Deep learning models (e.g., autoencoders, variational autoencoders) often outperform RBMs in many tasks due to better scalability and performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Output\n",
    "\n",
    "Running the provided code with the synthetic data might produce:\n",
    "```\n",
    "Epoch 0, Reconstruction Error: 0.2500\n",
    "Epoch 10, Reconstruction Error: 0.2200\n",
    "...\n",
    "Epoch 90, Reconstruction Error: 0.1800\n",
    "Original: [[1. 0. 1. 0. 0. 1. 1. 0. 1. 0.]]\n",
    "Reconstructed: [[0.9 0.1 0.8 0.2 0.1 0.9 0.8 0.1 0.9 0.2]]\n",
    "```\n",
    "\n",
    "This shows the RBM reconstructing the input with some noise, indicating it has learned the data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "RBMs are powerful for unsupervised learning tasks like recommender systems, feature learning, and anomaly detection. The provided Python code implements a binary RBM with Contrastive Divergence, suitable for small-scale datasets. For production use, consider libraries like TensorFlow or PyTorch for optimized implementations, or explore modern alternatives like variational autoencoders for better performance.\n",
    "\n",
    "If you have specific questions about the code, want to extend it for a particular use case (e.g., real-valued data), or need help with a dataset, let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbc4453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Reconstruction Error: 0.4940\n",
      "Epoch 10, Reconstruction Error: 0.5020\n",
      "Epoch 20, Reconstruction Error: 0.4890\n",
      "Epoch 30, Reconstruction Error: 0.4600\n",
      "Epoch 40, Reconstruction Error: 0.4750\n",
      "Epoch 50, Reconstruction Error: 0.4310\n",
      "Epoch 60, Reconstruction Error: 0.4270\n",
      "Epoch 70, Reconstruction Error: 0.4150\n",
      "Epoch 80, Reconstruction Error: 0.3760\n",
      "Epoch 90, Reconstruction Error: 0.4080\n",
      "Original: [[0. 1. 1. 1. 0. 0. 0. 1. 1. 1.]]\n",
      "Reconstructed: [[0. 1. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, n_visible, n_hidden, learning_rate=0.01, k=1):\n",
    "        \"\"\"\n",
    "        Initialize a Restricted Boltzmann Machine.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_visible: Number of visible units\n",
    "        - n_hidden: Number of hidden units\n",
    "        - learning_rate: Learning rate for weight updates\n",
    "        - k: Number of Gibbs sampling steps for Contrastive Divergence (CD-k)\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.k = k\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.normal(0, 0.01, (n_visible, n_hidden))  # Weight matrix\n",
    "        self.b = np.zeros(n_visible)  # Visible biases\n",
    "        self.c = np.zeros(n_hidden)   # Hidden biases\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Compute sigmoid activation.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sample_hidden(self, v):\n",
    "        \"\"\"\n",
    "        Sample hidden units given visible units.\n",
    "        Returns: Probabilities and binary states of hidden units.\n",
    "        \"\"\"\n",
    "        activation = np.dot(v, self.W) + self.c\n",
    "        p_h = self.sigmoid(activation)\n",
    "        h = (p_h > np.random.random(p_h.shape)).astype(float)\n",
    "        return p_h, h\n",
    "\n",
    "    def sample_visible(self, h):\n",
    "        \"\"\"\n",
    "        Sample visible units given hidden units.\n",
    "        Returns: Probabilities and binary states of visible units.\n",
    "        \"\"\"\n",
    "        activation = np.dot(h, self.W.T) + self.b\n",
    "        p_v = self.sigmoid(activation)\n",
    "        v = (p_v > np.random.random(p_v.shape)).astype(float)\n",
    "        return p_v, v\n",
    "\n",
    "    def contrastive_divergence(self, v):\n",
    "        \"\"\"\n",
    "        Perform one step of Contrastive Divergence (CD-k).\n",
    "        Returns: Positive and negative gradients for weights and biases.\n",
    "        \"\"\"\n",
    "        # Positive phase\n",
    "        pos_h_prob, pos_h = self.sample_hidden(v)\n",
    "        pos_assoc = np.dot(v.T, pos_h_prob)\n",
    "\n",
    "        # Negative phase (k steps of Gibbs sampling)\n",
    "        neg_v = v.copy()\n",
    "        for _ in range(self.k):\n",
    "            neg_h_prob, neg_h = self.sample_hidden(neg_v)\n",
    "            neg_v_prob, neg_v = self.sample_visible(neg_h)\n",
    "        \n",
    "        neg_assoc = np.dot(neg_v.T, neg_h_prob)\n",
    "\n",
    "        # Gradients\n",
    "        grad_W = pos_assoc - neg_assoc\n",
    "        grad_b = np.mean(v - neg_v, axis=0)\n",
    "        grad_c = np.mean(pos_h_prob - neg_h_prob, axis=0)\n",
    "\n",
    "        return grad_W, grad_b, grad_c\n",
    "\n",
    "    def train(self, data, epochs=100, batch_size=100):\n",
    "        \"\"\"\n",
    "        Train the RBM using Contrastive Divergence.\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Training data (binary, shape: [n_samples, n_visible])\n",
    "        - epochs: Number of training epochs\n",
    "        - batch_size: Size of mini-batches\n",
    "        \"\"\"\n",
    "        n_samples = data.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            error = 0\n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = min(start + batch_size, n_samples)\n",
    "                batch = data[start:end]\n",
    "\n",
    "                # Compute gradients\n",
    "                grad_W, grad_b, grad_c = self.contrastive_divergence(batch)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.W += self.learning_rate * grad_W / batch_size\n",
    "                self.b += self.learning_rate * grad_b\n",
    "                self.c += self.learning_rate * grad_c\n",
    "\n",
    "                # Compute reconstruction error\n",
    "                error += np.mean((batch - self.sample_visible(self.sample_hidden(batch)[1])[1])**2)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Reconstruction Error: {error / (n_samples // batch_size):.4f}\")\n",
    "\n",
    "    def reconstruct(self, v):\n",
    "        \"\"\"\n",
    "        Reconstruct input data by sampling hidden and then visible units.\n",
    "        Returns: Reconstructed visible units.\n",
    "        \"\"\"\n",
    "        _, h = self.sample_hidden(v)\n",
    "        _, v_reconstructed = self.sample_visible(h)\n",
    "        return v_reconstructed\n",
    "\n",
    "# Example usage with synthetic binary data\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic binary data (e.g., 100 samples with 10 visible units)\n",
    "    np.random.seed(42)\n",
    "    data = (np.random.random((100, 10)) > 0.5).astype(float)\n",
    "    \n",
    "    # Initialize and train RBM\n",
    "    rbm = RBM(n_visible=10, n_hidden=5, learning_rate=0.1, k=1)\n",
    "    rbm.train(data, epochs=100, batch_size=10)\n",
    "    \n",
    "    # Reconstruct a sample\n",
    "    sample = data[0:1]\n",
    "    reconstructed = rbm.reconstruct(sample)\n",
    "    print(\"Original:\", sample)\n",
    "    print(\"Reconstructed:\", reconstructed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
