{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajit-ai/DataScience/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvL6wd5cwYGZ"
      },
      "outputs": [],
      "source": [
        "%pip install sentence-transformers faiss-cpu transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NBRmD51wm_T"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "\n",
        "# Step 1: Prepare a small knowledge base (list of documents)\n",
        "documents = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"France is known for its wine and cheese.\",\n",
        "    \"The Eiffel Tower is in Paris, France.\",\n",
        "    \"Florida is a state in the USA, and its capital is Tallahassee.\"\n",
        "]\n",
        "\n",
        "# Step 2: Encode documents using Sentence Transformers\n",
        "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "doc_embeddings = encoder.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "# Step 3: Create a FAISS index for similarity search\n",
        "dimension = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "# Step 4: Define the query and retrieve relevant documents\n",
        "query = \"What is the capital of France?\"\n",
        "query_embedding = encoder.encode([query])\n",
        "\n",
        "# Search for top-k relevant documents\n",
        "k = 2\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Retrieve the relevant documents\n",
        "retrieved_docs = [documents[i] for i in indices[0]]\n",
        "print(\"Retrieved Documents:\", retrieved_docs)\n",
        "\n",
        "# Step 5: Use a generative model to produce the final answer\n",
        "generator = pipeline('text-generation', model='distilgpt2')\n",
        "context = f\"Question: {query}\\nContext: {' '.join(retrieved_docs)}\"\n",
        "prompt = f\"{context}\\nAnswer:\"\n",
        "response = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
        "\n",
        "# Extract and clean the answer\n",
        "answer = response.split(\"Answer:\")[1].strip() if \"Answer:\" in response else response\n",
        "print(\"Generated Answer:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9SiBgrEzM4X"
      },
      "outputs": [],
      "source": [
        "# prompt: generate a RAG codes for text\n",
        "\n",
        "# Function to perform RAG\n",
        "def answer_question_with_rag(query, encoder, index, documents, generator):\n",
        "    \"\"\"\n",
        "    Answers a question using the RAG approach.\n",
        "\n",
        "    Args:\n",
        "        query (str): The question to answer.\n",
        "        encoder (SentenceTransformer): The model used to encode documents and queries.\n",
        "        index (faiss.IndexFlatL2): The FAISS index for similarity search.\n",
        "        documents (list): The list of documents in the knowledge base.\n",
        "        generator (pipeline): The generative model.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated answer.\n",
        "    \"\"\"\n",
        "    # Step 4: Define the query and retrieve relevant documents\n",
        "    query_embedding = encoder.encode([query])\n",
        "\n",
        "    # Search for top-k relevant documents\n",
        "    k = 2  # You can adjust k\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Retrieve the relevant documents\n",
        "    retrieved_docs = [documents[i] for i in indices[0]]\n",
        "    print(\"Retrieved Documents:\", retrieved_docs)\n",
        "\n",
        "    # Step 5: Use a generative model to produce the final answer\n",
        "    context = f\"Question: {query}\\nContext: {' '.join(retrieved_docs)}\"\n",
        "    prompt = f\"{context}\\nAnswer:\"\n",
        "\n",
        "    # Use a reasonable max_length for the generated text\n",
        "    response = generator(prompt, max_length=100, num_return_sequences=1)[0]['generated_text']\n",
        "\n",
        "    # Extract and clean the answer\n",
        "    # Find the index of \"Answer:\" and take the text after it\n",
        "    answer_start_index = response.find(\"Answer:\")\n",
        "    if answer_start_index != -1:\n",
        "        answer = response[answer_start_index + len(\"Answer:\"):].strip()\n",
        "    else:\n",
        "        # If \"Answer:\" is not found, take the entire generated text and potentially clean it\n",
        "        answer = response.strip()\n",
        "        # Further cleaning to remove potential prompt remnants or incomplete sentences\n",
        "        # This part might need adjustment based on the generator's typical output\n",
        "        if '\\n' in answer:\n",
        "            answer = answer.split('\\n')[0] # Take the first line if it contains newlines\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage of the function\n",
        "new_query = \"Where is the Eiffel Tower?\"\n",
        "generated_answer = answer_question_with_rag(new_query, encoder, index, documents, generator)\n",
        "print(\"Generated Answer:\", generated_answer)\n",
        "\n",
        "new_query_2 = \"What is the capital of Florida?\"\n",
        "generated_answer_2 = answer_question_with_rag(new_query_2, encoder, index, documents, generator)\n",
        "print(\"Generated Answer:\", generated_answer_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOGnDTih1A5FCCqfdUpEL6D",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
