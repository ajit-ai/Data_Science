{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWxegzxlb5gXGRsqXu3dgT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajit-ai/DataScience/blob/main/GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is GloVe?**\n",
        "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm designed to generate dense vector representations also known as embeddings. Its primary objective is to capture semantic relationships between words by analyzing their co-occurrence patterns in a large text corpus.\n",
        "\n",
        "**GloVe approach is unique as it effectively combines the strengths of two major approaches:**\n",
        "\n",
        "Latent Semantic Analysis (LSA) which uses global statistical information.\n",
        "Context-based models like Word2Vec which focuses on local word context.\n",
        "At the core of GloVe lies the idea of mapping each word into a continuous vector space where both the magnitude and direction of the vectors reflect meaningful semantic relationships.\n",
        "\n",
        "For instance, the relationship captured in a vector equation can be like: king - man + woman = queen.\n",
        "\n",
        "To achieve this, GloVe constructs a word co-occurrence matrix where each element reflects how often a pair of words appears together within a given context window. It then optimizes the word vectors such that the dot product between any two word vectors approximates the pointwise mutual information (PMI) of the corresponding word pair. This optimization allows GloVe to produce embeddings that effectively encode both syntactic and semantic relationships across the vocabulary.\n",
        "\n",
        "**Glove Data**\n",
        "Glove has pre-defined dense vectors for around every 6 billion words of English literature along with many other general-use characters like commas, braces and semicolons. The algorithm's developers frequently make the pre-trained GloVe embeddings available. It is not necessary to train the model from scratch when using these pre-trained embeddings which can be downloaded and used immediately in a variety of natural language processing (NLP) applications. Users can select a pre-trained GloVe embedding in a dimension like 50-d, 100-d, 200-d or 300-d vectors that best fits their needs in terms of computational resources and task specificity.\n",
        "\n",
        "Here d stands for dimension. 100d means in this file each word has an equivalent vector of size 100. Glove files are simple text files in the form of a dictionary. Words are key and dense vectors are values of key.\n",
        "\n",
        "**How GloVe works?**\n",
        "The creation of a word co-occurrence matrix is the fundamental component of GloVe. This matrix provides a quantitative measure of the semantic affinity between words by capturing the frequency with which they appear together in a given context. Further, by minimising the difference between the dot product of vectors and the pointwise mutual information of corresponding words, GloVe optimises word vectors. It is able to produce dense vector representations that capture syntactic and semantic relationships.\n",
        "\n",
        "**Creating Vocabulary Dictionary**\n",
        "Vocabulary is the collection of all unique words present in the training dataset. The first dataset is tokenized into words, then all the frequency of each word is counted. Then words are sorted in decreasing order of their frequencies. Words having high frequency are placed at the beginning of the dictionary.\n",
        "\n",
        "Dataset= {The peon is ringing the bell}\n",
        "Vocabulary= {'The':2, 'peon':1, 'is':1, 'ringing':1}\n",
        "Algorithm for word embedding\n",
        "Preprocess the text data.\n",
        "Created the dictionary.\n",
        "Traverse the glove file of a specific dimension and compare each word with all words in the dictionary,\n",
        "if a match occurs, copy the equivalent vector from the glove and paste into embedding_matrix at the corresponding index."
      ],
      "metadata": {
        "id": "wV1deUvgnJ5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JHH5hzHTlcof"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample corpus\n",
        "texts = ['text', 'the', 'leader', 'prime', 'natural', 'language']"
      ],
      "metadata": {
        "id": "oT0wr57JlnKO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Output the word-index dictionary\n",
        "print(\"Number of unique words in dictionary =\", len(tokenizer.word_index))\n",
        "print(\"Dictionary is =\", tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h2OOOqIlqa0",
        "outputId": "693a1fcb-f3fb-4a4d-c942-9b77fe8790f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary = 6\n",
            "Dictionary is = {'text': 1, 'the': 2, 'leader': 3, 'prime': 4, 'natural': 5, 'language': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # +1 for padding token (index 0)\n",
        "    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word]\n",
        "                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix_vocab"
      ],
      "metadata": {
        "id": "ByDl2jMqlu99"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the GloVe dataset\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "# Unzip the file\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgC0vb1ply2N",
        "outputId": "0b578197-06b7-49ed-b497-e56d8a483f38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-08 05:51:55--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-07-08 05:51:55--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-07-08 05:51:55--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2025-07-08 05:54:34 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set embedding dimension (match this with glove file)\n",
        "embedding_dim = 50\n",
        "\n",
        "# Path to GloVe file\n",
        "glove_path = './glove.6B.50d.txt'\n",
        "\n",
        "# Generate embedding matrix\n",
        "embedding_matrix_vocab = embedding_for_vocab(glove_path, tokenizer.word_index, embedding_dim)"
      ],
      "metadata": {
        "id": "oPIDhaksmwVe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dense vector for the first word in the tokenizer index\n",
        "first_word_index = 1  # Tokenizer indexes start from 1\n",
        "print(\"Dense vector for word with index 1 =>\", embedding_matrix_vocab[first_word_index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx3dXUqUm2Jl",
        "outputId": "4c302ac0-727e-4a46-dbbd-5fd9793e3288"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense vector for word with index 1 => [ 0.32615     0.36686    -0.0074905  -0.37553     0.66715002  0.21646\n",
            " -0.19801    -1.10010004 -0.42221001  0.10574    -0.31292     0.50953001\n",
            "  0.55774999  0.12019     0.31441    -0.25042999 -1.06369996 -1.32130003\n",
            "  0.87797999 -0.24627     0.27379    -0.51091999  0.49324     0.52243\n",
            "  1.16359997 -0.75322998 -0.48052999 -0.11259    -0.54595    -0.83920997\n",
            "  2.98250008 -1.19159997 -0.51958001 -0.39365    -0.1419     -0.026977\n",
            "  0.66295999  0.16574    -1.1681      0.14443     1.63049996 -0.17216\n",
            " -0.17436001 -0.01049    -0.17794     0.93076003  1.0381      0.94265997\n",
            " -0.14805    -0.61109   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe Embeddings Applications\n",
        "GloVe embeddings are a popular option for representing words in text data and have found applications in various natural language processing (NLP) tasks. The following are some typical uses for GloVe embeddings:\n",
        "\n",
        "Text Classification: GloVe embeddings can be utilised as features in machine learning models for sentiment analysis, topic classification, spam detection and other applications.\n",
        "Named Entity Recognition (NER): By capturing the semantic relationships between words and enhancing the model's capacity to identify entities in text, GloVe embeddings can improve the performance of NER systems.\n",
        "Machine Translation: GloVe embeddings can be used to represent words in the source and target languages in machine translation systems which aim to translate text from one language to another, thereby enhancing the quality of the translation.\n",
        "Question Answering Systems: To help models understand the context between words and produce more accurate answers, GloVe embeddings are used in question-answering tasks.\n",
        "Document Similarity and Clustering: GloVe embeddings enable applications in information retrieval and document organization by measuring the semantic similarity between documents or grouping documents according to their content.\n",
        "Word Analogy Tasks: In word analogy tasks, GloVe embeddings frequently yield good results. For instance, the generated vector for \"king-man + woman\" might resemble the \"queen\" vector, demonstrating the capacity to recognize semantic relationships.\n",
        "Semantic Search: In semantic search applications where retrieving documents or passages according to their semantic relevance to a user's query is the aim, GloVe embeddings are helpful.\n"
      ],
      "metadata": {
        "id": "LjIAxM0YnU39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In modern Natural Language Processing (NLP), understanding and processing human language in a machine-readable format is essential. Since machines interpret numbers, it's important to convert textual data into numerical form. One of the most effective and widely used approaches to achieve this is through word embeddings."
      ],
      "metadata": {
        "id": "HOiCDd-1m_du"
      }
    }
  ]
}