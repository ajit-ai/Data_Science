{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c539829a",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors \n",
    "\n",
    "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm generally used for classification but can also be used for regression tasks. It works by finding the \"k\" closest data points (neighbors) to a given input and makesa predictions based on the majority class (for classification) or the average value (for regression). Since KNN makes no assumptions about the underlying data distribution it makes it a non-parametric and instance-based learning method.\n",
    "\n",
    "Calculate-Data.webpCalculate-Data.webp\n",
    "K-Nearest Neighbors is also called as a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification it performs an action on the dataset.\n",
    "\n",
    "For example, consider the following table of data points containing two features:\n",
    "\n",
    "KNN Algorithm working visualization\n",
    "KNN Algorithm working visualization\n",
    "The new point is classified as Category 2 because most of its closest neighbors are blue squares. KNN assigns the category based on the majority of nearby points. The image shows how KNN predicts the category of a new data point based on its closest neighbours.\n",
    "\n",
    "The red diamonds represent Category 1 and the blue squares represent Category 2.\n",
    "The new data point checks its closest neighbors (circled points).\n",
    "Since the majority of its closest neighbors are blue squares (Category 2) KNN predicts the new data point belongs to Category 2.\n",
    "KNN works by using proximity and majority voting to make predictions.\n",
    "\n",
    "What is 'K' in K Nearest Neighbour?\n",
    "In the k-Nearest Neighbours algorithm k is just a number that tells the algorithm how many nearby points or neighbors to look at when it makes a decision.\n",
    "\n",
    "Example: Imagine you're deciding which fruit it is based on its shape and size. You compare it to fruits you already know.\n",
    "\n",
    "If k = 3, the algorithm looks at the 3 closest fruits to the new one.\n",
    "If 2 of those 3 fruits are apples and 1 is a banana, the algorithm says the new fruit is an apple because most of its neighbors are apples.\n",
    "How to choose the value of k for KNN Algorithm?\n",
    "The value of k in KNN decides how many neighbors the algorithm looks at when making a prediction.\n",
    "Choosing the right k is important for good results.\n",
    "If the data has lots of noise or outliers, using a larger k can make the predictions more stable.\n",
    "But if k is too large the model may become too simple and miss important patterns and this is called underfitting.\n",
    "So k should be picked carefully based on the data.\n",
    "Statistical Methods for Selecting k\n",
    "Cross-Validation: Cross-Validation is a good way to find the best value of k is by using k-fold cross-validation. This means dividing the dataset into k parts. The model is trained on some of these parts and tested on the remaining ones. This process is repeated for each part. The k value that gives the highest average accuracy during these tests is usually the best one to use.\n",
    "Elbow Method: In Elbow Method we draw a graph showing the error rate or accuracy for different k values. As k increases the error usually drops at first. But after a certain point error stops decreasing quickly. The point where the curve changes direction and looks like an \"elbow\" is usually the best choice for k.\n",
    "Odd Values for k: It’s a good idea to use an odd number for k especially in classification problems. This helps avoid ties when deciding which class is the most common among the neighbors.\n",
    "Distance Metrics Used in KNN Algorithm\n",
    "KNN uses distance metrics to identify nearest neighbor, these neighbors are used for classification and regression task. To identify nearest neighbor we use below distance metrics:\n",
    "\n",
    "1. Euclidean Distance\n",
    "Euclidean distance is defined as the straight-line distance between two points in a plane or space. You can think of it like the shortest path you would walk if you were to go directly from one point to another.\n",
    "\n",
    "distance\n",
    "(\n",
    "x\n",
    ",\n",
    "X\n",
    "i\n",
    ")\n",
    "=\n",
    "∑\n",
    "j\n",
    "=\n",
    "1\n",
    "d\n",
    "(\n",
    "x\n",
    "j\n",
    "−\n",
    "X\n",
    "i\n",
    "j\n",
    ")\n",
    "2\n",
    "]\n",
    "distance(x,X \n",
    "i\n",
    "​\n",
    " )= \n",
    "∑ \n",
    "j=1\n",
    "d\n",
    "​\n",
    " (x \n",
    "j\n",
    "​\n",
    " −X \n",
    "i \n",
    "j\n",
    "​\n",
    " \n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "​\n",
    " ]\n",
    "\n",
    "2. Manhattan Distance\n",
    "This is the total distance you would travel if you could only move along horizontal and vertical lines like a grid or city streets. It’s also called \"taxicab distance\" because a taxi can only drive along the grid-like streets of a city.\n",
    "\n",
    "d\n",
    "(\n",
    "x\n",
    ",\n",
    "y\n",
    ")\n",
    "=\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    "n\n",
    "∣\n",
    "x\n",
    "i\n",
    "−\n",
    "y\n",
    "i\n",
    "∣\n",
    "d(x,y)=∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣x \n",
    "i\n",
    "​\n",
    " −y \n",
    "i\n",
    "​\n",
    " ∣\n",
    "\n",
    "3. Minkowski Distance\n",
    "Minkowski distance is like a family of distances, which includes both Euclidean and Manhattan distances as special cases.\n",
    "\n",
    "d\n",
    "(\n",
    "x\n",
    ",\n",
    "y\n",
    ")\n",
    "=\n",
    "(\n",
    "∑\n",
    "i\n",
    "=\n",
    "1\n",
    "n\n",
    "(\n",
    "x\n",
    "i\n",
    "−\n",
    "y\n",
    "i\n",
    ")\n",
    "p\n",
    ")\n",
    "1\n",
    "p\n",
    "d(x,y)=(∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " −y \n",
    "i\n",
    "​\n",
    " ) \n",
    "p\n",
    " ) \n",
    "p\n",
    "1\n",
    "​\n",
    " \n",
    " \n",
    "\n",
    "From the formula above, when p=2, it becomes the same as the Euclidean distance formula and when p=1, it turns into the Manhattan distance formula. Minkowski distance is essentially a flexible formula that can represent either Euclidean or Manhattan distance depending on the value of p.\n",
    "\n",
    "Working of KNN algorithm\n",
    "Thе K-Nearest Neighbors (KNN) algorithm operates on the principle of similarity where it predicts the label or value of a new data point by considering the labels or values of its K nearest neighbors in the training dataset.\n",
    "\n",
    "Workings of KNN algorithm\n",
    "\n",
    "Step 1: Selecting the optimal value of K\n",
    "K represents the number of nearest neighbors that needs to be considered while making prediction.\n",
    "Step 2: Calculating distance\n",
    "To measure the similarity between target and training data points Euclidean distance is used. Distance is calculated between data points in the dataset and target point.\n",
    "Step 3: Finding Nearest Neighbors\n",
    "The k data points with the smallest distances to the target point are nearest neighbors.\n",
    "Step 4: Voting for Classification or Taking Average for Regression\n",
    "When you want to classify a data point into a category like spam or not spam, the KNN algorithm looks at the K closest points in the dataset. These closest points are called neighbors. The algorithm then looks at which category the neighbors belong to and picks the one that appears the most. This is called majority voting.\n",
    "In regression, the algorithm still looks for the K closest points. But instead of voting for a class in classification, it takes the average of the values of those K neighbors. This average is the predicted value for the new point for the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0352284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6a4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((np.array(point1) - np.array(point2))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972ac48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(training_data, training_labels, test_point, k):\n",
    "    distances = []\n",
    "    for i in range(len(training_data)):\n",
    "        dist = euclidean_distance(test_point, training_data[i])\n",
    "        distances.append((dist, training_labels[i]))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    k_nearest_labels = [label for _, label in distances[:k]]\n",
    "    return Counter(k_nearest_labels).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20b08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [[1, 2], [2, 3], [3, 4], [6, 7], [7, 8]]\n",
    "training_labels = ['A', 'A', 'A', 'B', 'B']\n",
    "test_point = [4, 5]\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5052888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "prediction = knn_predict(training_data, training_labels, test_point, k)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
